port: 3000

# Configure ProcessorChains to alter requests
processors:
  simpleChain:
    # Converts every system message to user role
    - type: "nosys"
    # Ensures there are no "dangling" sys messages, that is, no sys messages after the initial sysprompt block
    # Redundant with the previous processor but it's daijobu
    - type: "nodanglingsys"
    - type: "overridesamplers"
      topP: 0.9
      temperature: "unset" # unsets whatever temp value the request sent in
    - type: "regex" # replaces foo with bar
      pattern: "foo"
      flags: "gi"
      replacement: "bar"

# Configure KeyProviders to ensure your ModelProviders have Keys
keyProviders:
  yourORKey:
    type: "literal"
    key: "sk-or-made-up-key"
  yourOtherORKey:
    type: "environment"
    envVar: "MY_OTHER_OR_KEY"

modelProviders:
  openrouter:
    type: "genericoai"
    url: "https://openrouter.ai/api/v1"
    keyProvider: yourORKey
    models:
      # This exposes a model named "gpt-4.1" in your client
      gpt-4.1:
        type: "openrouter" # OpenRouter model
        name: "openai/gpt-4.1" # Its model name on OpenRouter
      # This exposes a model named "gpt-4.1-two" in your client. It's the same as 4.1, but two.
      gpt-4.1-two:
        type: "openrouter" # OpenRouter model
        name: "openai/gpt-4.1" # Its model name on OpenRouter

  llamacpp:
    type: "genericoai"
    url: "http://localhost:8000/v1"
    keyProvider:  # You NEED to still give him a fake made up key (or he'll cry)
      type: "literal"
      key: "sk-i-made-it-up"
    models:
      qwen-3-32b:
        name: "qwen/qwen3-32b-q8-gguf-something-something"
        processor: simpleChain

  random:
    type: "random" # Randomly chooses between the other models you have configured!
    keyProvider:  # EVEN IF HE'S A random PROVIDER YOU GIVE HIM THE MADE UP KEY
      type: "literal"
      key: "sk-i-made-it-up"
    modelList:
      - openrouter/gpt-4.1
      - llamacpp/qwen-3-32b

  random-2:
    type: "random"
    keyProvider:
      type: "literal"
      key: "sk-i-made-it-up"
    modelWeights: # 5/21 requests go to gpt-4.1 as configured above, 16/21 goes to qwen-3-32b.
      "openrouter/gpt-4.1": 5
      "llamacpp/qwen-3-32b": 16 # You can use floats and your weights don't have to sum to anything. Just non-zero + positive.
