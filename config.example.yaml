port: 3000

# Configure ProcessorChains to alter requests
processorChains:
  simpleChain:
    # Converts every system message to user role
    - type: "nosys"
    # Ensures there are no "dangling" sys messages, that is, no sys messages after the initial sysprompt block
    # Redundant with the previous processor but it's daijobu
    - type: "nodanglingsys"
    - type: "overridesamplers"
      topP: 0.9
      temperature: "unset" # unsets whatever temp value the request sent in
    - type: "regex" # replaces foo with bar
      pattern: "foo"
      flags: "gi"
      replacement: "bar"

# Configure KeyProviders to ensure your ModelProviders have Keys
keyProviders:
  yourORKey:
    type: "literal"
    key: "sk-or-made-up-key"
    modelTargets:
      - ".*" # matches all model names
  yourOtherORKey:
    type: "environment"
    envVar: "MY_OTHER_OR_KEY"
    modelTargets:
      - "regex that will match NOTHING lol"

modelProviders:
  # This exposes a model named "gpt-4.1" in your client
  gpt-4.1:
    type: "openrouter" # OpenRouter model
    modelName: "openai/gpt-4.1" # Its model name on OpenRouter
  # This exposes a model named "gpt-4.1-two" in your client. It's the same as 4.1, but two.
  gpt-4.1-two:
    type: "openrouter" # OpenRouter model
    modelName: "openai/gpt-4.1" # Its model name on OpenRouter
  qwen-3-32b:
    type: "genericoai" # Anything that largely resembles OAI
    url: "http://localhost:8000/v1/chat/completions" # e.g. your local llamacpp server. You NEED to still give him a fake made up key (or he'll cry)
    modelName: "qwen/qwen3-32b-q8-gguf-something-something"
    processorChain: simpleChain
  random:
    type: "random" # Randomly chooses between the other models you have configured!
    modelList:
      - gpt-4.1
      - qwen-3-32b
  random-2:
    type: "random"
    modelWeights: # 5/21 requests go to gpt-4.1 as configured above, 16/21 goes to qwen-3-32b.
      "gpt-4.1": 5
      "qwen-3-32b": 16 # You can use floats and your weights don't have to sum to anything. Just non-zero + positive.
